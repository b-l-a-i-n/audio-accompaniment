Организация проекта
------------
Наш проект разделен на две части. 
Первая ml_service иммитирует микросервис, осуществляющий работу с моделями ии Image2Text и Image Detection. Реализовано только подключение и предсказание на предобученных моделях с hugging face. 
Вторая часть предстовляет backend+frontend, реализовано также через Fastapi. За основу взят плеер shaka-player.
Обе части проекта развернуты в docker контейнерах. 

Запуск проекта
------------
Запуск проекта происходит через docker-compose файлы, расположенные в каждой из частей проекта. Контейнеры работают как изолированные микросервисы и  общаются между собой через Rest Api. 
Ссылка на сервер, где собран и запущен проект:
http://91.185.84.110/

Описание идеи
------------
Основная идея проекта - погрузить человека с нарушениями работы зрительной системы в среду фильма и создать большее вовлечение в цеонтент путем озвучивания происходящего.

Описание работы сервиса
------------
Структурная схема архитектуры сервиса отражена на слайдах. 
В определенные моменты времени (на данный момент реализовано по нажатию кнопки), когда человеку может быть непонятно происходящее (или мы думаем что будет не понятно), посылается запрос на бэкэнд, и затем на микросервис с моделями ИИ, в виде таймкода, на сервере этот запрос обрабатывается, из фильма вырезается конкретный кадр (или несколько предшествующих, для лучшего предсказания) и по нему с помощью Image2Text модели предсказывается что происходит на экране и также с помощью модели Object Detection выделяются объекты на кадре для улучшения восприятия. И далее ответы двух моделей совмещаются и отправляются ответом на фронтенд, конвертируются в аудио с помощью экранного диктора и воспроизводятся пользователю. 

Преимущества
------------
1. Real Time обработка
2. Скорость обработки запроса ~ 3-5 секунд на серверах MTS Cloud
3. Из пункта 2 следует еще один плюс - устойчивость к нагрузкам ввиду малой скорости обработки. 
4 Минимальные вычислительные затраты (кроме ML моделей), не надо тратить вычислительные мощности на рендер
5 Наш проект может существовать отдельно как микросервис, отдельно от плеера и применен в других областях, к примеру описывать людям с проблемами зрения что изображено на картинке в соцсети. 

Улучшение и масштабирование
------------
1. Можно написать алгоритм (возможно на основе нейросетей), который будет автоматически определять, где людям необходимо сделать подсказки и входя в accessibility режим плеера, заранее генерировать таймкоды и воспроизводить пользователю. Если это будет нейросеть, но понять когда людям нужна подсказка, можно на основе онлайн метрик, анализа когда люди чаще всего нажимают на кнопку подсказки что происходит на экране

2. Реализовать, чтобы каждая модель ИИ работала отдельно, т. е. асинхронно, тогда уменьшится время, требуемое для обработки запроса. Также подойти адекватнее к развертыванию (ML Flow и другие фреймворки, позволяющие автоматизировать работу моделей). Также можно делать файнтюн моделей под конкретный жанр фильма, и тогда человек будет больше погружаться в происходящее. 

3. Масштабирование в основном происходит по пункту 1, когда мы записываем в буфер описание отдельных частей фильма. Как можно заметить, предикт занимает минимальное количество времени, с учетом фактора неоптимизированности нашего решения в данной реалзизации. 

4. Необходимо улучшить фронтенд, добавить описание в тэги описание, чтобы незрячие люди, пользующиеся voice over при переходе на страницу узнали о данной возможности. Также добавить управление запросами к сервису с помощью горячих клавиш (незрячие люди работают только с клавиатурой, и горячие клавиши для них основное средство общения со страницей в интернете)
