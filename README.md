Организация проекта
------------
Наш проект разделен на две части. 
Первая ml_service иммитирует микросервис, осуществляющий работу с моделями ии Image2Text и Image Detection. Реализовано только подключение и предсказание на предобученных моделях с hugging face. 
Вторая часть предстовляет backend+frontend, реализовано также через Fastapi. За основу взят плеер shaka-player.

Запуск проекта
------------
Запуск проекта происходит через docker-compose файлы, расположенные в каждой из частей проекта. Контейнеры работают как изолированные микросервисы и  общаются между собой через Rest Api. 
Ссылка на сервер, где собран и запущен проект:
http://91.185.84.110/

Описание идеи
------------
Основная идея проекта - погрузить человека с нарушениями работы зрительной системы в среду фильма и создать большее вовлечение в цеонтент путем озвучивания происходящего.

Описание работы сервиса
------------
В определенные моменты времени (на данный момент реализовано по нажатию кнопки), когда человеку может быть непонятно происходящее (или мы думаем что будет не понятно), посылается запрос на бэкэнд, и затем на микросервис с моделями ИИ, в виде таймкода, на сервере этот запрос обрабатывается, из фильма вырезается конкретный кадр (или несколько предшествующих, для лучшего предсказания) и по нему с помощью Image2Text модели предсказывается что происходит на экране и также с помощью модели Object Detection выделяются объекты на кадре для улучшения восприятия. И далее ответы двух моделей совмещаются и отправляются ответом на фронтенд, конвертируются в аудио с помощью экранного диктора и воспроизводятся пользователю. 

Преимущества
------------
1. Real Time обработка
2. Скорость обработки запроса ~ 3-5 секунд на серверах MTS Cloud
3. Минимальные вычислительные затраты (кроме ML моделей), не надо тратить вычислительные мощности на рендер
4. Наш проект может существовать отдельно от конкретного плеера и применен в других областях, к примеру описывать людям с проблемами зрения что изображено на картинке в соцсети. 

Улучшение и масштабирование
------------
1. Можно написать алгоритм (возможно на основе нейросетей), который будет автоматически определять, где людям необходимо сделать подсказки и входя в accessibility режим плеера, заранее генерировать таймкоды и воспроизводить пользователю. Если это будет нейросеть, но понять когда людям нужна подсказка, можно на основе онлайн метрик, анализа когда люди чаще всего нажимают на кнопку подсказки что происходит на экране

2. Реализовать, чтобы каждая модель ИИ работала отдельно, т. е. асинхронно, тогда уменьшится время, требуемое для обработки запроса. Также подойти адекватнее к развертыванию (ML Flow и другие фреймворки, позволяющие автоматизировать работу моделей).
